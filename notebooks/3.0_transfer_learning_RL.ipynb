{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning models:\n",
    "\n",
    "This notebook is about transfer learning models and the reinforcement learning agent to determine whether to use the transfer learner or an active learning policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import src.active_learning as al\n",
    "import src.viz as viz\n",
    "import src.reinforcement as rl\n",
    "import src.data as d \n",
    "from src.models import logreg, CNN, AgentRL\n",
    "# import active_learning as al\n",
    "# import viz\n",
    "# import reinforcement as rl\n",
    "# import data as d \n",
    "# from models import logreg, CNN, AgentRL\n",
    "\n",
    "import importlib as imp\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision.models as tmodels\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw datasets - MNIST\n",
    "d = imp.reload(d)\n",
    "train_set = dset.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download=False)\n",
    "test_set = dset.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=len(test_set),shuffle=False)\n",
    "\n",
    "# Get raw dataset - USPS\n",
    "percent_test = 0.3\n",
    "usps_batch = 64\n",
    "usps_set = d.get_usps('usps/usps_all.mat', size=(28,28))\n",
    "usps_x, usps_y, usps_test_x, usps_test_y = al.get_dataset_split(usps_set,int(len(usps_set)*percent_test))\n",
    "usps_test_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(usps_test_x, usps_test_y), \\\n",
    "                                               batch_size=len(usps_test_y),shuffle=False)\n",
    "usps_train_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(usps_x, usps_y), \\\n",
    "                                               batch_size=usps_batch,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x/y split for the test set \n",
    "train_x, train_y, val_x, val_y = al.get_dataset_split(train_set)\n",
    "test_x,test_y = al.get_xy_split(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get resnet from pytorch (trained on imagenet)\n",
    "model_in = tmodels.resnet18(pretrained=True)\n",
    "# model_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulblankley/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/paulblankley/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS training accuracy: 0.966493506494\n",
      "USPS test accuracy: 0.953939393939\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained usps handwritten classifier \n",
    "# Warnings causes by pytorch versioning issues with 0.3.1 on conda GPU vs 0.3.0 on conda CPU\n",
    "model_usps = torch.load('paul_models/usps_model.pt')\n",
    "print('USPS training accuracy:',al.accuracy(model_usps, usps_x, usps_y))\n",
    "print('USPS test accuracy:',al.accuracy(model_usps, usps_test_x, usps_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the transfer learners are loaded we are ready to reinforcement learn:\n",
    "\n",
    "for a given state of the model.. meaning all potential training points, labeled and unlabeled and our predictions on those, is it better to active learn with policy $y$ and continue training our original model or is it better to transfer a different model and retrain the final layer.\n",
    "\n",
    "## Q's for Pavlos:\n",
    "\n",
    "How should we try to integrate the different transfer agent? Do we just check its performance on points we miss with the other? How should we evaluate its contribution (provided it has any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the RL agent to interact with the environment \n",
    "class AgentRL(nn.Module):\n",
    "    def __init__(self, inpt_dim, hidden_dim, num_policies):\n",
    "        super(AgentRL, self).__init__()\n",
    "        self.num_policies = num_policies\n",
    "        self.inner_layer = nn.Linear(inpt_dim, hidden_dim)\n",
    "        self.outer_layer = nn.Linear(hidden_dim, num_policies)\n",
    "        self.rewards = []\n",
    "        self.saved_log_probs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(1,-1)\n",
    "        x = F.relu(self.inner_layer(x))\n",
    "        x = self.outer_layer(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRL(int(len(train_x)*10),128, 6) # 6 for the 5 AL policies and one for the TL policy\n",
    "optimizer_rl = optim.Adam(agent.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
